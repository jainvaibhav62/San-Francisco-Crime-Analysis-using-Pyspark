{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "From 1934 to 1963, San Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of Alcatraz.\n",
    "\n",
    "Today, the city is known more for its tech scene than its criminal past. But, with rising wealth inequality, housing shortages, and a proliferation of expensive digital toys riding BART to work, there is no scarcity of crime in the city by the bay.\n",
    "\n",
    "From Sunset to SOMA, and Marina to Excelsior, this competition's dataset provides nearly 12 years of crime reports from across all of San Francisco's neighborhoods. Given time and location, you must predict the category of crime that occurred.\n",
    "\n",
    "We're also encouraging you to explore the dataset visually. What can we learn about the city through visualizations like this Top Crimes Map? The top most up-voted scripts from this competition will receive official Kaggle swag as prizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data fields\n",
    "Dates - timestamp of the crime incident\n",
    "\n",
    "Category - category of the crime incident (only in train.csv). This is the target variable you are going to predict.\n",
    "\n",
    "Descript - detailed description of the crime incident (only in train.csv)\n",
    "\n",
    "DayOfWeek - the day of the week\n",
    "\n",
    "PdDistrict - name of the Police Department District\n",
    "\n",
    "Resolution - how the crime incident was resolved (only in train.csv)\n",
    "\n",
    "Address - the approximate street address of the crime incident \n",
    "\n",
    "X - Longitude\n",
    "\n",
    "Y - Latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('sf_crime_classification').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "|              Dates|      Category|            Descript|DayOfWeek|PdDistrict|    Resolution|             Address|                  X|                 Y|\n",
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "|2015-05-13 23:53:00|      WARRANTS|      WARRANT ARREST|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|\n",
      "|2015-05-13 23:53:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|\n",
      "|2015-05-13 23:33:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|VANNESS AV / GREE...|   -122.42436302145|  37.8004143219856|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  NORTHERN|          NONE|1500 Block of LOM...|-122.42699532676599| 37.80087263276921|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|      PARK|          NONE|100 Block of BROD...|  -122.438737622757|37.771541172057795|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday| INGLESIDE|          NONE| 0 Block of TEDDY AV|-122.40325236121201|   37.713430704116|\n",
      "|2015-05-13 23:30:00| VEHICLE THEFT|   STOLEN AUTOMOBILE|Wednesday| INGLESIDE|          NONE| AVALON AV / PERU AV|  -122.423326976668|  37.7251380403778|\n",
      "|2015-05-13 23:30:00| VEHICLE THEFT|   STOLEN AUTOMOBILE|Wednesday|   BAYVIEW|          NONE|KIRKWOOD AV / DON...|  -122.371274317441|  37.7275640719518|\n",
      "|2015-05-13 23:00:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  RICHMOND|          NONE|600 Block of 47TH AV|  -122.508194031117|37.776601260681204|\n",
      "|2015-05-13 23:00:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|   CENTRAL|          NONE|JEFFERSON ST / LE...|  -122.419087676747|  37.8078015516515|\n",
      "|2015-05-13 22:58:00| LARCENY/THEFT|PETTY THEFT FROM ...|Wednesday|   CENTRAL|          NONE|JEFFERSON ST / LE...|  -122.419087676747|  37.8078015516515|\n",
      "|2015-05-13 22:30:00|OTHER OFFENSES|MISCELLANEOUS INV...|Wednesday|   TARAVAL|          NONE|0 Block of ESCOLT...|  -122.487983072777|37.737666654332706|\n",
      "|2015-05-13 22:30:00|     VANDALISM|MALICIOUS MISCHIE...|Wednesday|TENDERLOIN|          NONE|  TURK ST / JONES ST|-122.41241426358101|  37.7830037964534|\n",
      "|2015-05-13 22:06:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  NORTHERN|          NONE|FILLMORE ST / GEA...|  -122.432914603494|  37.7843533426568|\n",
      "|2015-05-13 22:00:00|  NON-CRIMINAL|      FOUND PROPERTY|Wednesday|   BAYVIEW|          NONE|200 Block of WILL...|  -122.397744427103|  37.7299346936044|\n",
      "|2015-05-13 22:00:00|  NON-CRIMINAL|      FOUND PROPERTY|Wednesday|   BAYVIEW|          NONE|0 Block of MENDEL...|-122.38369150395901|  37.7431890419965|\n",
      "|2015-05-13 22:00:00|       ROBBERY|ROBBERY, ARMED WI...|Wednesday|TENDERLOIN|          NONE|  EDDY ST / JONES ST|  -122.412597377187|37.783932027727296|\n",
      "|2015-05-13 21:55:00|       ASSAULT|AGGRAVATED ASSAUL...|Wednesday| INGLESIDE|          NONE|GODEUS ST / MISSI...|  -122.421681531572|  37.7428222004845|\n",
      "|2015-05-13 21:40:00|OTHER OFFENSES|   TRAFFIC VIOLATION|Wednesday|   BAYVIEW|ARREST, BOOKED|MENDELL ST / HUDS...|-122.38640086995301|   37.738983491072|\n",
      "|2015-05-13 21:30:00|  NON-CRIMINAL|      FOUND PROPERTY|Wednesday|TENDERLOIN|          NONE|100 Block of JONE...|  -122.412249767634|   37.782556330202|\n",
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "df = spark.read.csv('train.csv',header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Structure\n",
      "----------------------------------\n",
      "root\n",
      " |-- Dates: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- PdDistrict: string (nullable = true)\n",
      " |-- Resolution: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- X: string (nullable = true)\n",
      " |-- Y: string (nullable = true)\n",
      "\n",
      "None\n",
      " \n",
      "Dataframe preview\n",
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "|              Dates|      Category|            Descript|DayOfWeek|PdDistrict|    Resolution|             Address|                  X|                 Y|\n",
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "|2015-05-13 23:53:00|      WARRANTS|      WARRANT ARREST|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|\n",
      "|2015-05-13 23:53:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|\n",
      "|2015-05-13 23:33:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|VANNESS AV / GREE...|   -122.42436302145|  37.8004143219856|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  NORTHERN|          NONE|1500 Block of LOM...|-122.42699532676599| 37.80087263276921|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|      PARK|          NONE|100 Block of BROD...|  -122.438737622757|37.771541172057795|\n",
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      " \n",
      "----------------------------------\n",
      "Total number of rows 878049\n"
     ]
    }
   ],
   "source": [
    "print('Dataframe Structure')\n",
    "print('----------------------------------')\n",
    "print(df.printSchema())\n",
    "print(' ')\n",
    "print('Dataframe preview')\n",
    "print(df.show(5))\n",
    "print(' ')\n",
    "print('----------------------------------')\n",
    "print('Total number of rows', df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dates',\n",
       " 'Category',\n",
       " 'Descript',\n",
       " 'DayOfWeek',\n",
       " 'PdDistrict',\n",
       " 'Resolution',\n",
       " 'Address',\n",
       " 'X',\n",
       " 'Y']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "def top_n_list(df,var, N):\n",
    "    '''\n",
    "    This function determine the top N numbers of the list\n",
    "    '''\n",
    "    print(\"Total number of unique value of\"+' '+var+''+':'+' '+str(df.select(var).distinct().count()),'\\n')\n",
    "    print('Top'+' '+str(N)+' '+'Crime'+' '+var)\n",
    "    \n",
    "    df.groupBy(var).count().withColumnRenamed('count','totalValue')\\\n",
    "    .orderBy(col('totalValue').desc()).show(N)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique value of Category: 39 \n",
      "\n",
      "Top 10 Crime Category\n",
      "+--------------+----------+\n",
      "|      Category|totalValue|\n",
      "+--------------+----------+\n",
      "| LARCENY/THEFT|    174900|\n",
      "|OTHER OFFENSES|    126182|\n",
      "|  NON-CRIMINAL|     92304|\n",
      "|       ASSAULT|     76876|\n",
      "| DRUG/NARCOTIC|     53971|\n",
      "| VEHICLE THEFT|     53781|\n",
      "|     VANDALISM|     44725|\n",
      "|      WARRANTS|     42214|\n",
      "|      BURGLARY|     36755|\n",
      "|SUSPICIOUS OCC|     31414|\n",
      "+--------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_n_list(df, 'Category',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique value of Descript: 879 \n",
      "\n",
      "Top 10 Crime Descript\n",
      "+--------------------+----------+\n",
      "|            Descript|totalValue|\n",
      "+--------------------+----------+\n",
      "|GRAND THEFT FROM ...|     60022|\n",
      "|       LOST PROPERTY|     31729|\n",
      "|             BATTERY|     27441|\n",
      "|   STOLEN AUTOMOBILE|     26897|\n",
      "|DRIVERS LICENSE, ...|     26839|\n",
      "|      WARRANT ARREST|     23754|\n",
      "|SUSPICIOUS OCCURR...|     21891|\n",
      "|AIDED CASE, MENTA...|     21497|\n",
      "|PETTY THEFT FROM ...|     19771|\n",
      "|MALICIOUS MISCHIE...|     17789|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_n_list(df,'Descript',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+---------+----------+----------+-------+---+---+\n",
      "|Dates|Category|Descript|DayOfWeek|PdDistrict|Resolution|Address|  X|  Y|\n",
      "+-----+--------+--------+---------+----------+----------+-------+---+---+\n",
      "|    0|       0|       0|        0|         0|         0|      0|  0|  0|\n",
      "+-----+--------+--------+---------+----------+----------+-------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "df.select(\n",
    "   [count(when(col(i).isNull() , i)).alias(i) for i in df.columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=====================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+--------------------+---------+----------+--------------+--------------------+--------------------+-------------------+\n",
      "|summary|              Dates|   Category|            Descript|DayOfWeek|PdDistrict|    Resolution|             Address|                   X|                  Y|\n",
      "+-------+-------------------+-----------+--------------------+---------+----------+--------------+--------------------+--------------------+-------------------+\n",
      "|  count|             878049|     878049|              878049|   878049|    878049|        878049|              878049|              878049|             878049|\n",
      "|   mean|               null|       null|                null|     null|      null|          null|                null| -122.42261645502985|  37.77102029801277|\n",
      "| stddev|               null|       null|                null|     null|      null|          null|                null|0.030353622998489052|0.45689310470581795|\n",
      "|    min|2003-01-06 00:01:00|      ARSON|ABANDONMENT OF CHILD|   Friday|   BAYVIEW|ARREST, BOOKED|0 Block of  HARRI...|              -120.5|   37.7078790224135|\n",
      "|    max|2015-05-13 23:53:00|WEAPON LAWS|         YOUTH COURT|Wednesday|TENDERLOIN|     UNFOUNDED|   ZOE ST / WELSH ST|    -122.51364206429|               90.0|\n",
      "+-------+-------------------+-----------+--------------------+---------+----------+--------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"sf_crime_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dates',\n",
       " 'Category',\n",
       " 'Descript',\n",
       " 'DayOfWeek',\n",
       " 'PdDistrict',\n",
       " 'Resolution',\n",
       " 'Address',\n",
       " 'X',\n",
       " 'Y']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|distinct_categories|\n",
      "+-------------------+\n",
      "|                 39|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_query = spark.sql(\n",
    "    '''\n",
    "    Select COUNT(distinct Category) as distinct_categories\n",
    "    from\n",
    "    sf_crime_classification\n",
    "    '''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|min(Dates)|max(Dates)|\n",
      "+----------+----------+\n",
      "|2003-01-06|2015-05-13|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_query = spark.sql(\n",
    "    '''\n",
    "    Select DATE(min(Dates)), DATE(max(Dates))\n",
    "    from\n",
    "    sf_crime_classification\n",
    "    '''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sql\n",
    "#     Select DATE(min(Dates)), DATE(max(Dates))\n",
    "#     from\n",
    "#     sf_crime_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 614457\n",
      "Test Dataset Count: 263592\n"
     ]
    }
   ],
   "source": [
    "training, test = df.randomSplit([0.7,0.3], seed=60)\n",
    "#trainingSet.cache()\n",
    "print(\"Training Dataset Count:\", training.count())\n",
    "print(\"Test Dataset Count:\", test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, OneHotEncoder, StringIndexer, VectorAssembler, HashingTF, IDF, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes \n",
    "\n",
    "#----------------Define tokenizer with regextokenizer()------------------\n",
    "regex_tokenizer = RegexTokenizer(pattern='\\\\W')\\\n",
    "                  .setInputCol(\"Descript\")\\\n",
    "                  .setOutputCol(\"tokens\")\n",
    "\n",
    "#----------------Define stopwords with stopwordsremover()---------------------\n",
    "extra_stopwords = ['http','amp','rt','t','c','the']\n",
    "stopwords_remover = StopWordsRemover()\\\n",
    "                    .setInputCol('tokens')\\\n",
    "                    .setOutputCol('filtered_words')\\\n",
    "                    .setStopWords(extra_stopwords)\n",
    "                    \n",
    "\n",
    "#----------Define bags of words using countVectorizer()---------------------------\n",
    "count_vectors = CountVectorizer(vocabSize=10000, minDF=5)\\\n",
    "               .setInputCol(\"filtered_words\")\\\n",
    "               .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "#-----------Using TF-IDF to vectorise features instead of countVectoriser-----------------\n",
    "hashingTf = HashingTF(numFeatures=10000)\\\n",
    "            .setInputCol(\"filtered_words\")\\\n",
    "            .setOutputCol(\"raw_features\")\n",
    "            \n",
    "#Use minDocFreq to remove sparse terms\n",
    "idf = IDF(minDocFreq=5)\\\n",
    "        .setInputCol(\"raw_features\")\\\n",
    "        .setOutputCol(\"features\")\n",
    "\n",
    "#---------------Define bag of words using Word2Vec---------------------------\n",
    "word2Vec = Word2Vec(vectorSize=1000, minCount=0)\\\n",
    "           .setInputCol(\"filtered_words\")\\\n",
    "           .setOutputCol(\"features\")\n",
    "\n",
    "#-----------Encode the Category variable into label using StringIndexer-----------\n",
    "label_string_idx = StringIndexer()\\\n",
    "                  .setInputCol(\"Category\")\\\n",
    "                  .setOutputCol(\"label\")\n",
    "\n",
    "#-----------Define classifier structure for logistic Regression--------------\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "#---------Define classifier structure for Naive Bayes----------\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "\n",
    "def metrics_ev(labels, metrics):\n",
    "    '''\n",
    "    List of all performance metrics\n",
    "    '''\n",
    "    # Confusion matrix\n",
    "    print(\"---------Confusion matrix-----------------\")\n",
    "    print(metrics.confusionMatrix)\n",
    "    print(' ')    \n",
    "    # Overall statistics\n",
    "    print('----------Overall statistics-----------')\n",
    "    print(\"Precision = %s\" %  metrics.precision())\n",
    "    print(\"Recall = %s\" %  metrics.recall())\n",
    "    print(\"F1 Score = %s\" % metrics.fMeasure())\n",
    "    print(' ')\n",
    "    # Statistics by class\n",
    "    print('----------Statistics by class----------')\n",
    "    for label in sorted(labels):\n",
    "       print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "       print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "       print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "    print(' ')\n",
    "    # Weighted stats\n",
    "    print('----------Weighted stats----------------')\n",
    "    print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "    print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "    print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "    print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "    print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 20:18:44 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 20:18:45 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/12/01 20:18:45 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 20:18:46 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/12/01 20:18:46 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "pipeline_cv_lr = Pipeline().setStages([regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, lr])\n",
    "model_cv_lr = pipeline_cv_lr.fit(training)\n",
    "predictions_cv_lr = model_cv_lr.transform(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------Check Top 5 predictions----------------------------------\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 96:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|                      Descript|     Category|                   probability|label|prediction|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8711720612177544,0.02172...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8711720612177544,0.02172...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8711720612177544,0.02172...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8711720612177544,0.02172...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8711720612177544,0.02172...|  0.0|       0.0|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('-----------------------------Check Top 5 predictions----------------------------------')\n",
    "print(' ')\n",
    "predictions_cv_lr.select('Descript','Category',\"probability\",\"label\",\"prediction\")\\\n",
    "                                        .orderBy(\"probability\", ascending=False)\\\n",
    "                                        .show(n=5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "------------------------------Accuracy----------------------------------\n",
      " \n",
      "                       accuracy:0.9722709356739532:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
    "evaluator_cv_lr = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_cv_lr)\n",
    "print(' ')\n",
    "print('------------------------------Accuracy----------------------------------')\n",
    "print(' ')\n",
    "print('                       accuracy:{}:'.format(evaluator_cv_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Secondary model using NaiveBayes\n",
    "pipeline_cv_nb = Pipeline().setStages([regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, nb])\n",
    "model_cv_nb = pipeline_cv_nb.fit(training)\n",
    "predictions_cv_nb = model_cv_nb.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------Accuracy-----------------------------\n",
      " \n",
      "                      accuracy:0.9936147254042839:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator_cv_nb = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_cv_nb)\n",
    "print(' ')\n",
    "print('--------------------------Accuracy-----------------------------')\n",
    "print(' ')\n",
    "print('                      accuracy:{}:'.format(evaluator_cv_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline_idf_lr = Pipeline().setStages([regex_tokenizer,stopwords_remover,hashingTf, idf, label_string_idx, lr])\n",
    "model_idf_lr = pipeline_idf_lr.fit(training)\n",
    "predictions_idf_lr = model_idf_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------Check Top 5 predictions----------------------------------\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 160:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|                      Descript|     Category|                   probability|label|prediction|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8831671122887976,0.01976...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8831671122887976,0.01976...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8831671122887976,0.01976...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8831671122887976,0.01976...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8831671122887976,0.01976...|  0.0|       0.0|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('-----------------------------Check Top 5 predictions----------------------------------')\n",
    "print(' ')\n",
    "predictions_idf_lr.select('Descript','Category',\"probability\",\"label\",\"prediction\")\\\n",
    "                                        .orderBy(\"probability\", ascending=False)\\\n",
    "                                        .show(n=5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 161:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "-------------------------------Accuracy---------------------------------\n",
      " \n",
      "                        accuracy:0.9722324196105095:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator_idf_lr = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_idf_lr)\n",
    "print(' ')\n",
    "print('-------------------------------Accuracy---------------------------------')\n",
    "print(' ')\n",
    "print('                        accuracy:{}:'.format(evaluator_idf_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline_idf_nb = Pipeline().setStages([regex_tokenizer,stopwords_remover,hashingTf, idf, label_string_idx, nb])\n",
    "model_idf_nb = pipeline_idf_nb.fit(training)\n",
    "predictions_idf_nb = model_idf_nb.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 20:19:28 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 171:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "-----------------------------Accuracy-----------------------------\n",
      " \n",
      "                          accuracy:0.9949693209189158:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator_idf_nb = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_idf_nb)\n",
    "print(' ')\n",
    "print('-----------------------------Accuracy-----------------------------')\n",
    "print(' ')\n",
    "print('                          accuracy:{}:'.format(evaluator_idf_nb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
